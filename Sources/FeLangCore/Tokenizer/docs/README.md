# Tokenizer Module

The **Tokenizer Module** is responsible for breaking input text into tokens for parsing in the FE pseudo-language toolkit.

## ğŸ“ Module Structure

```
Tokenizer/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ README.md                 # This file - module overview
â”‚   â”œâ”€â”€ ARCHITECTURE.md           # Internal architecture details
â”‚   â”œâ”€â”€ PERFORMANCE.md            # Performance optimizations and benchmarks
â”‚   â””â”€â”€ EXAMPLES.md              # Usage examples and patterns
â”œâ”€â”€ Token.swift                   # Core token data structure
â”œâ”€â”€ TokenType.swift              # Token type enumeration
â”œâ”€â”€ SourcePosition.swift         # Source position tracking
â”œâ”€â”€ Tokenizer.swift              # Full-featured tokenizer
â”œâ”€â”€ ParsingTokenizer.swift       # Lightweight parsing tokenizer
â”œâ”€â”€ TokenizerError.swift         # Specialized error types
â””â”€â”€ TokenizerUtilities.swift     # Shared utilities and keyword maps
```

## ğŸ¯ Purpose

Converts raw source text into a stream of tokens that can be consumed by parsers. Provides two implementations:

- **Tokenizer**: Full-featured tokenizer with comprehensive error reporting
- **ParsingTokenizer**: Lightweight tokenizer optimized for parsing performance

## ğŸ”§ Core Components

### **Token.swift**
Core token data structure containing:
- `type`: TokenType enumeration value
- `lexeme`: Original text representation
- `literal`: Parsed literal value (for literals)
- `position`: Source position information

### **TokenType.swift**
Comprehensive enumeration of all token types:
- Keywords (English & Japanese): `if`/`ã‚‚ã—`, `else`/`ãã†ã§ãªã‘ã‚Œã°`, etc.
- Operators: `+`, `-`, `*`, `/`, `=`, `==`, etc.
- Literals: integers, reals, strings, characters, booleans
- Punctuation: `(`, `)`, `[`, `]`, `{`, `}`, `,`, `;`
- Special: `EOF`, `newline`, `whitespace`

### **SourcePosition.swift**
Tracks position information for debugging and error reporting:
- Line and column numbers
- Character offset
- Efficient position tracking during tokenization

## ğŸ“– Usage Examples

### **Basic Tokenization**
```swift
import FeLangCore

let tokenizer = Tokenizer(input: "x â† 1 + 2")
let tokens = try tokenizer.tokenize()

for token in tokens {
    print("\(token.type): '\(token.lexeme)' at \(token.position)")
}
```

### **Parsing-Optimized Tokenization**
```swift
let parsingTokenizer = ParsingTokenizer(input: "if x == 1 then y â† 2")

while !parsingTokenizer.isAtEnd {
    let token = try parsingTokenizer.nextToken()
    // Process token immediately
}
```

### **Error Handling**
```swift
do {
    let tokens = try tokenizer.tokenize()
} catch TokenizerError.unexpectedCharacter(let char, let position) {
    print("Unexpected character '\(char)' at \(position)")
} catch TokenizerError.unterminatedString(let position) {
    print("Unterminated string at \(position)")
}
```

## ğŸ” Key Features

### **Multi-Language Support**
```swift
// English keywords
let englishCode = "if x > 0 then y â† x"

// Japanese keywords  
let japaneseCode = "ã‚‚ã— x > 0 ãªã‚‰ y â† x"

// Both produce identical token streams
```

### **Comprehensive Literal Support**
```swift
// Integer literals
"42", "0", "-123"

// Real literals
"3.14", "0.5", "-2.718"

// String literals with escape sequences
"\"Hello\\nWorld\"", "'c'"

// Boolean literals
"true", "false", "çœŸ", "å½"
```

### **Robust Error Detection**
- Invalid characters
- Unterminated strings
- Malformed numbers
- Position tracking for all errors

## âš¡ Performance Features

### **Efficient Character Processing**
- Single-pass tokenization
- Minimal string allocations
- Optimized keyword lookup using hash maps

### **Memory Management**
- Token reuse where possible
- Efficient position tracking
- Minimal object creation

### **Parser Integration**
- `ParsingTokenizer` provides on-demand tokenization
- Reduces memory usage for large files
- Supports streaming tokenization patterns

## ğŸ›¡ï¸ Security Features

### **Input Validation**
- Character encoding validation
- Maximum input size limits
- Safe string processing

### **Error Boundaries**
- Controlled error propagation
- Safe handling of malformed input
- Position-aware error reporting

## ğŸ”— Dependencies

### **Internal Dependencies**
- None - Tokenizer is the foundation layer

### **External Dependencies**
- Swift Foundation (String, Character processing)

## ğŸ§ª Testing

The Tokenizer module has comprehensive test coverage:
- **TokenizerTests.swift**: Core tokenizer functionality
- **ParsingTokenizerTests.swift**: Parser-optimized tokenizer
- **TokenizerConsistencyTests.swift**: Cross-implementation consistency
- **LeadingDotTests.swift**: Edge case handling

See **[../../../Tests/FeLangCoreTests/Tokenizer/](../../../Tests/FeLangCoreTests/Tokenizer/)** for complete test suite.

## ğŸ“š Additional Documentation

- **[ARCHITECTURE.md](ARCHITECTURE.md)**: Internal architecture and design decisions
- **[PERFORMANCE.md](PERFORMANCE.md)**: Performance optimizations and benchmarks
- **[EXAMPLES.md](EXAMPLES.md)**: Advanced usage examples and patterns

---

The **Tokenizer Module** provides the foundation for all text processing in FeLangCore! ğŸš€ 