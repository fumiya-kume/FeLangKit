import Foundation
import Testing
@testable import FeLangCore

/// Comprehensive tests for Unicode normalization functionality
/// Tests Japanese character handling, full-width conversion, and combining characters
@Suite("Unicode Normalization Tests")
struct UnicodeNormalizationTests {

    private let normalizer = UnicodeNormalizer()

    // MARK: - Basic Normalization Tests

    @Test("Basic NFC Normalization")
    func testBasicNFCNormalization() throws {
        // Test combining character normalization (dakuten)
        let decomposed = "„Åå" // „Åå as separate characters („Åã + combining dakuten)
        let composed = "„Åå"   // „Åå as single composed character

        let normalizedDecomposed = UnicodeNormalizer.normalizeForFE(decomposed)
        let normalizedComposed = UnicodeNormalizer.normalizeForFE(composed)

        #expect(normalizedDecomposed == normalizedComposed, "NFC normalization should produce identical results")
        #expect(normalizer.areEquivalent(decomposed, composed), "Should recognize equivalent characters")
    }

    @Test("Full-width to Half-width Conversion")
    func testFullwidthToHalfwidthConversion() throws {
        let fullwidthText = "Ôº°Ôº¢Ôº£ÔºëÔºíÔºìÔºÅÔºü"
        let expectedHalfwidth = "ABC123!?"

        let normalized = UnicodeNormalizer.normalizeForFE(fullwidthText)

        #expect(normalized == expectedHalfwidth, "Full-width ASCII should convert to half-width")
    }

    @Test("Mixed Full-width and Half-width Text")
    func testMixedFullwidthHalfwidthText() throws {
        let mixedText = "Â§âÊï∞Ôº∂Ôº°Ôº≤ÔºùÔºëÔºíÔºì"
        let expectedResult = "Â§âÊï∞VAR=123"

        let normalized = UnicodeNormalizer.normalizeForFE(mixedText)

        #expect(normalized == expectedResult, "Should normalize only ASCII full-width characters")
    }

    // MARK: - Japanese Character Specific Tests

    @Test("Dakuten and Handakuten Normalization")
    func testDakutenHandakutenNormalization() throws {
        let testCases = [
            ("„Åã\u{3099}", "„Åå"), // „Åã + combining dakuten ‚Üí „Åå
            ("„ÅØ\u{309A}", "„Å±"), // „ÅØ + combining handakuten ‚Üí „Å±
            ("„Åï\u{3099}", "„Åñ"), // „Åï + combining dakuten ‚Üí „Åñ
            ("„ÅÜ\u{3099}", "„É¥") // „ÅÜ + combining dakuten ‚Üí „É¥
        ]

        for (decomposed, expectedComposed) in testCases {
            let normalized = UnicodeNormalizer.normalizeForFE(decomposed)
            #expect(normalized == expectedComposed, "Combining marks should be normalized to composed characters")
        }
    }

    @Test("Katakana vs Hiragana Consistency")
    func testKatakanaHiraganaConsistency() throws {
        let hiraganaText = "„Å∏„Çç„Éº"
        let katakanaText = "„Éò„É≠„Éº"

        // Both should remain as-is (no automatic conversion)
        let normalizedHiragana = UnicodeNormalizer.normalizeForFE(hiraganaText)
        let normalizedKatakana = UnicodeNormalizer.normalizeForFE(katakanaText)

        #expect(normalizedHiragana == hiraganaText, "Hiragana should remain unchanged")
        #expect(normalizedKatakana == katakanaText, "Katakana should remain unchanged")
        #expect(normalizedHiragana != normalizedKatakana, "Hiragana and Katakana should remain distinct")
    }

    @Test("Japanese Punctuation Normalization")
    func testJapanesePunctuationNormalization() throws {
        let testCases = [
            ("ÔΩû", "~"),   // Wave dash
            ("‚àí", "-"),   // Minus sign
            ("‚Äï", "‚Äî")   // EM dash
        ]

        for (original, expected) in testCases {
            let normalized = UnicodeNormalizer.normalizeForFE(original)
            #expect(normalized == expected, "Japanese punctuation should be normalized")
        }
    }

    // MARK: - Complex Text Tests

    @Test("Complex Mixed Text Normalization")
    func testComplexMixedTextNormalization() throws {
        let complexText = "Â§âÊï∞„ÄÄÔº∂Ôº°Ôº≤ÔºøÔºë„ÄÄÔºù„ÄÄ„Äå„Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå„ÄçÔºãÔºëÔºíÔºì"
        let expectedResult = "Â§âÊï∞„ÄÄVAR_1„ÄÄ=„ÄÄ„Äå„Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå„Äç+123"

        let normalized = UnicodeNormalizer.normalizeForFE(complexText)

        #expect(normalized == expectedResult, "Complex mixed text should be properly normalized")
    }

    @Test("Programming Keywords with Full-width Characters")
    func testProgrammingKeywordsNormalization() throws {
        let fullwidthKeywords = "ÔΩâÔΩÜ„ÄÄÔΩîÔΩàÔΩÖÔΩé„ÄÄÔΩÖÔΩåÔΩìÔΩÖ„ÄÄÔΩÖÔΩéÔΩÑÔΩâÔΩÜ"
        let expectedResult = "if„ÄÄthen„ÄÄelse„ÄÄendif"

        let normalized = UnicodeNormalizer.normalizeForFE(fullwidthKeywords)

        #expect(normalized == expectedResult, "Programming keywords should be normalized")
    }

    // MARK: - Statistics and Analysis Tests

    @Test("Normalization Statistics")
    func testNormalizationStatistics() throws {
        let testText = "Ôº∂Ôº°Ôº≤ÔºøÔºëÔºíÔºì„ÄÄ„Åã\u{3099}"
        let stats = normalizer.analyzeNormalization(testText)

        #expect(stats.hasChanges, "Should detect changes in the text")
        #expect(stats.fullwidthCharactersConverted > 0, "Should count full-width characters")
        #expect(stats.combiningCharactersNormalized > 0, "Should count combining characters")
        #expect(stats.originalLength >= stats.normalizedLength, "Normalized text should not be longer")
    }

    @Test("No Changes Statistics")
    func testNoChangesStatistics() throws {
        let plainText = "hello world 123"
        let stats = normalizer.analyzeNormalization(plainText)

        #expect(!stats.hasChanges, "Plain ASCII text should not require changes")
        #expect(stats.fullwidthCharactersConverted == 0, "No full-width characters to convert")
        #expect(stats.combiningCharactersNormalized == 0, "No combining characters to normalize")
        #expect(stats.originalLength == stats.normalizedLength, "Lengths should be equal")
    }

    // MARK: - Instance-based Statistics Tracking Tests

    @Test("Instance Statistics Tracking - Full-width Characters")
    func testInstanceStatisticsTrackingFullwidth() throws {
        var normalizer = UnicodeNormalizer()
        let testText = "Ôº°Ôº¢Ôº£Ôº§ÔºëÔºíÔºìÔºî"

        let normalized = normalizer.normalize(testText)
        let stats = normalizer.getStats()

        #expect(normalized == "ABCD1234", "Should normalize full-width characters")
        #expect(stats.originalLength == 8, "Should track original length")
        #expect(stats.normalizedLength == 8, "Should track normalized length")
        #expect(stats.fullwidthConversions == 8, "Should count 8 full-width conversions")
        #expect(stats.nfcNormalizations == 0, "Should count 0 NFC changes for this input")
        #expect(stats.japaneseNormalizations == 0, "Should count 0 Japanese normalizations")
        #expect(stats.emojiNormalizations == 0, "Should count 0 emoji normalizations")
        #expect(stats.mathSymbolNormalizations == 0, "Should count 0 math symbol normalizations")
        #expect(!stats.hasSecurityConcerns, "Should not have security concerns")
    }

    @Test("Instance Statistics Tracking - Japanese Characters")
    func testInstanceStatisticsTrackingJapanese() throws {
        var normalizer = UnicodeNormalizer()
        let testText = "„Åì„Çì„Å´„Å°„ÅØ„Äú‰∏ñÁïåÔºÅ‚àí„Çî"

        let normalized = normalizer.normalize(testText)
        let stats = normalizer.getStats()

        #expect(stats.japaneseNormalizations == 3, "Should count 3 Japanese normalizations: „Äú, ‚àí, „Çî")
        #expect(stats.fullwidthConversions == 1, "Should count 1 full-width conversion: ÔºÅ")
        #expect(stats.originalLength > 0, "Should track original length")
        #expect(stats.normalizedLength > 0, "Should track normalized length")
    }

    @Test("Instance Statistics Tracking - Mixed Content")
    func testInstanceStatisticsTrackingMixed() throws {
        var normalizer = UnicodeNormalizer()
        let testText = "ÔºÆÔº°Ôº≠Ôº•„Åå„ÄúÔºëÔºíÔºì"

        let normalized = normalizer.normalize(testText)
        let stats = normalizer.getStats()

        #expect(normalized == "NAME„Åå~123", "Should properly normalize mixed content")
        #expect(stats.fullwidthConversions == 7, "Should count NAME123 conversions")
        #expect(stats.japaneseNormalizations == 1, "Should count wave dash normalization")
        #expect(stats.compressionRatio == 1.0, "Length should remain the same")
    }

    @Test("Statistics Reset Functionality")
    func testStatisticsResetFunctionality() throws {
        var normalizer = UnicodeNormalizer()

        // First normalization
        _ = normalizer.normalize("Ôº°Ôº¢Ôº£Ôº§")
        let firstStats = normalizer.getStats()
        #expect(firstStats.fullwidthConversions == 4, "Should track first normalization")

        // Reset and normalize again
        normalizer.resetStats()
        _ = normalizer.normalize("ÔºëÔºíÔºì")
        let secondStats = normalizer.getStats()

        #expect(secondStats.fullwidthConversions == 3, "Should track only second normalization after reset")
        #expect(secondStats.originalLength == 3, "Should reset original length")
    }

    @Test("String Extension with Statistics")
    func testStringExtensionWithStatistics() throws {
        let testText = "Ôº®Ôº•Ôº¨Ôº¨ÔºØ„Åå„Äú"
        let (normalized, stats) = testText.normalizedForFEWithStats()

        #expect(normalized == "HELLO„Åå~", "Should normalize correctly")
        #expect(stats.fullwidthConversions == 5, "Should count HELLO conversions")
        #expect(stats.japaneseNormalizations == 1, "Should count wave dash")
        #expect(stats.originalLength == 7, "Should track original length")
        #expect(stats.normalizedLength == 7, "Should track normalized length")
    }

    @Test("Statistics Accuracy - Complex Example")
    func testStatisticsAccuracyComplexExample() throws {
        var normalizer = UnicodeNormalizer()
        let complexText = "Â§âÊï∞„ÄÄÔºÆÔº°Ôº≠Ôº•„Åå„ÄÄÔºù„ÄÄ\"Ôº®Ôº•Ôº¨Ôº¨ÔºØ„ÄúÔºÅ\"‚àíÔºëÔºíÔºì"

        let normalized = normalizer.normalize(complexText)
        let stats = normalizer.getStats()

        // Count expected changes:
        // Full-width: NAME + = + HELLO + ! + 123 = 4 + 1 + 5 + 1 + 3 = 14
        // Japanese: „Äú + ‚àí = 2
        #expect(stats.fullwidthConversions == 14, "Should count all full-width conversions accurately")
        #expect(stats.japaneseNormalizations == 2, "Should count Japanese normalizations accurately")

        // Verify the actual result
        #expect(normalized.contains("NAME"), "Should convert NAME")
        #expect(normalized.contains("HELLO"), "Should convert HELLO")
        #expect(normalized.contains("~"), "Should convert wave dash")
        #expect(normalized.contains("-"), "Should convert minus")
    }

    // MARK: - Edge Cases and Error Handling

    @Test("Empty String Normalization")
    func testEmptyStringNormalization() throws {
        let empty = ""
        let normalized = UnicodeNormalizer.normalizeForFE(empty)

        #expect(normalized == empty, "Empty string should remain empty")
        #expect(normalizer.areEquivalent(empty, normalized), "Empty strings should be equivalent")
    }

    @Test("Single Character Normalization")
    func testSingleCharacterNormalization() throws {
        let singleChar = "A"
        let fullwidthSingle = "Ôº°"

        let normalizedRegular = UnicodeNormalizer.normalizeForFE(singleChar)
        let normalizedFullwidth = UnicodeNormalizer.normalizeForFE(fullwidthSingle)

        #expect(normalizedRegular == singleChar, "Regular character should remain unchanged")
        #expect(normalizedFullwidth == "A", "Full-width character should be converted")
        #expect(normalizer.areEquivalent(singleChar, fullwidthSingle), "Should recognize equivalence")
    }

    @Test("Very Long Text Normalization")
    func testVeryLongTextNormalization() throws {
        let longText = String(repeating: "Â§âÊï∞„ÄÄÔº∂Ôº°Ôº≤ÔºøÔºëÔºíÔºì„ÄÄ„Åã\u{3099}„ÄÄ", count: 1000)

        let startTime = CFAbsoluteTimeGetCurrent()
        let normalized = UnicodeNormalizer.normalizeForFE(longText)
        let processingTime = CFAbsoluteTimeGetCurrent() - startTime

        #expect(!normalized.isEmpty, "Should handle long text")
        #expect(processingTime < 1.0, "Should process long text reasonably quickly")

        let stats = normalizer.analyzeNormalization(longText)
        #expect(stats.hasChanges, "Should detect changes in repeated text")
    }

    // MARK: - Convenience Extension Tests

    @Test("String Extension Convenience Method")
    func testStringExtensionConvenience() throws {
        let testText = "Ôº®Ôº•Ôº¨Ôº¨ÔºØ„Åã\u{3099}"
        let normalizedDirect = UnicodeNormalizer.normalizeForFE(testText)
        let normalizedExtension = testText.normalizedForFE

        #expect(normalizedDirect == normalizedExtension, "Extension method should produce same result")
    }

    // MARK: - Real-world FE Language Examples

    @Test("FE Language Variable Declaration")
    func testFELanguageVariableDeclaration() throws {
        let feCode = "Â§âÊï∞„ÄÄÔºÆÔº°Ôº≠Ôº•„Åã\u{3099}„ÄÄÔºù„ÄÄ\"Ôº®Ôº•Ôº¨Ôº¨ÔºØ„Åã\u{3099}\";"
        let expectedNormalized = "Â§âÊï∞„ÄÄNAME„Åå„ÄÄ=„ÄÄ\"HELLO„Åå\";"

        let normalized = UnicodeNormalizer.normalizeForFE(feCode)

        #expect(normalized == expectedNormalized, "FE language code should be properly normalized")
    }

    @Test("FE Language Control Flow")
    func testFELanguageControlFlow() throws {
        let feCode = """
        ÔΩâÔΩÜ„ÄÄÔΩò„ÄÄÔºû„ÄÄÔºê„ÄÄÔΩîÔΩàÔΩÖÔΩé
            Â§âÊï∞„ÄÄÔΩíÔΩÖÔΩìÔΩïÔΩåÔΩî„ÄÄÔºù„ÄÄ\"ÊàêÂäü„Åã\u{3099}\"
        ÔΩÖÔΩéÔΩÑÔΩâÔΩÜ
        """

        let normalized = UnicodeNormalizer.normalizeForFE(feCode)

        #expect(normalized.contains("if"), "Keywords should be normalized")
        #expect(normalized.contains("then"), "Keywords should be normalized")
        #expect(normalized.contains("endif"), "Keywords should be normalized")
        #expect(normalized.contains("ÊàêÂäü„Åå"), "Japanese text should be normalized")
    }

    // MARK: - Performance Tests

    @Test("Normalization Performance")
    func testNormalizationPerformance() throws {
        let sampleTexts = [
            "Â§âÊï∞ name = \"hello\"",
            "Ôº∂Ôº°Ôº≤„ÄÄÔΩéÔΩÅÔΩçÔΩÖ„ÄÄÔºù„ÄÄ\"ÔΩàÔΩÖÔΩåÔΩåÔΩè\"",
            "„Åã\u{3099}„Åç\u{3099}„Åè\u{3099}„Åë\u{3099}„Åì\u{3099}",
            String(repeating: "„ÉÜ„Çπ„ÉàÔº¥Ôº•Ôº≥Ôº¥", count: 100)
        ]

        for text in sampleTexts {
            let iterations = 1000
            let startTime = CFAbsoluteTimeGetCurrent()

            for _ in 0..<iterations {
                _ = UnicodeNormalizer.normalizeForFE(text)
            }

            let totalTime = CFAbsoluteTimeGetCurrent() - startTime
            let avgTime = totalTime / Double(iterations)

            #expect(avgTime < 0.001, "Normalization should be fast (< 1ms per operation)")
        }
    }

    // MARK: - New Enhanced Features Tests

    @Test("Normalization Forms - NFD Testing")
    func testNormalizationFormsNFD() throws {
        var normalizer = UnicodeNormalizer()
        
        // Test NFD (decomposition) - use explicit composed character
        let composedText = "caf\u{00E9}"  // √© as single composed character (U+00E9)
        let result = normalizer.normalize(composedText, form: .nfd)
        
        // NFD should decompose √© into e + combining acute
        // Note: grapheme cluster count stays the same, but Unicode scalar count increases
        #expect(result.unicodeScalars.count > composedText.unicodeScalars.count, "NFD should decompose characters")
        #expect(result.unicodeScalars.contains(UnicodeScalar(0x0301)!), "Should contain combining acute accent")
    }

    @Test("Normalization Forms - NFKC Testing")
    func testNormalizationFormsNFKC() throws {
        var normalizer = UnicodeNormalizer()
        
        // Test NFKC with compatibility characters
        let compatText = "Ô¨Åle"  // fi ligature
        let result = normalizer.normalize(compatText, form: .nfkc)
        
        #expect(result == "file", "NFKC should decompose compatibility characters")
    }

    @Test("Normalization Forms - NFKD Testing")
    func testNormalizationFormsNFKD() throws {
        var normalizer = UnicodeNormalizer()
        
        // Test NFKD with compatibility and decomposition
        let compatText = "caf√©" // with compatibility characters
        let result = normalizer.normalize(compatText, form: .nfkd)
        
        // Should apply both compatibility and canonical decomposition
        #expect(result.count >= compatText.count, "NFKD should decompose compatibility and canonical")
    }

    @Test("Character Classification System")
    func testCharacterClassificationSystem() throws {
        // Test letter classification
        let letterA = UnicodeScalar(65)! // 'A'
        let classA = UnicodeNormalizer.classifyCharacter(letterA)
        if case .letter(let subcategory) = classA {
            #expect(subcategory == .uppercaseLetter, "A should be classified as uppercase letter")
        } else {
            #expect(Bool(false), "A should be classified as letter")
        }

        // Test number classification
        let digit5 = UnicodeScalar(53)! // '5'
        let class5 = UnicodeNormalizer.classifyCharacter(digit5)
        if case .number(let subcategory) = class5 {
            #expect(subcategory == .decimalDigitNumber, "5 should be classified as decimal digit")
        } else {
            #expect(Bool(false), "5 should be classified as number")
        }

        // Test mathematical symbol
        let piSymbol = UnicodeScalar(0x03C0)! // œÄ
        let classPi = UnicodeNormalizer.classifyCharacter(piSymbol)
        if case .symbol(let subcategory) = classPi {
            #expect(subcategory == .mathSymbol, "œÄ should be classified as math symbol")
        } else {
            #expect(Bool(false), "œÄ should be classified as symbol")
        }

        // Test punctuation
        let openParen = UnicodeScalar(40)! // '('
        let classParen = UnicodeNormalizer.classifyCharacter(openParen)
        if case .punctuation(let subcategory) = classParen {
            #expect(subcategory == .openPunctuation, "( should be classified as open punctuation")
        } else {
            #expect(Bool(false), "( should be classified as punctuation")
        }
    }

    @Test("Mathematical Symbol Normalization")
    func testMathematicalSymbolNormalization() throws {
        var normalizer = UnicodeNormalizer()
        
        let mathText = "œÄ √ó Œ± √∑ Œ≤ ‚âà ‚àû"
        let result = normalizer.normalize(mathText)
        let stats = normalizer.getStats()
        
        #expect(result.contains("pi"), "œÄ should be normalized to pi")
        #expect(result.contains("*"), "√ó should be normalized to *")
        #expect(result.contains("alpha"), "Œ± should be normalized to alpha")
        #expect(result.contains("/"), "√∑ should be normalized to /")
        #expect(result.contains("beta"), "Œ≤ should be normalized to beta")
        #expect(result.contains("~="), "‚âà should be normalized to ~=")
        #expect(result.contains("infinity"), "‚àû should be normalized to infinity")
        #expect(stats.mathSymbolNormalizations > 0, "Should count math symbol normalizations")
    }

    @Test("Emoji Normalization")
    func testEmojiNormalization() throws {
        var normalizer = UnicodeNormalizer()
        
        // Test emoji with variation selectors
        let emojiText = "üòÄ\u{FE0F}üëã\u{FE0E}"
        let result = normalizer.normalize(emojiText)
        let stats = normalizer.getStats()
        
        #expect(!result.contains("\u{FE0F}"), "Should remove emoji variation selector")
        #expect(!result.contains("\u{FE0E}"), "Should remove text variation selector")
        #expect(stats.emojiNormalizations > 0, "Should count emoji normalizations")
    }

    @Test("Security - Homoglyph Detection")
    func testSecurityHomoglyphDetection() throws {
        var normalizer = UnicodeNormalizer()
        
        // Test Cyrillic homoglyphs
        let homoglyphText = "–∞—Å–µ"  // Cyrillic a, c, e that look like Latin
        let result = normalizer.normalize(homoglyphText)
        let stats = normalizer.getStats()
        
        #expect(result == "ace", "Should convert Cyrillic homoglyphs to Latin")
        #expect(stats.homoglyphsDetected > 0, "Should detect homoglyphs")
        #expect(stats.hasSecurityConcerns, "Should flag security concerns")
    }

    @Test("Security - Bidirectional Text Protection")
    func testSecurityBidirectionalTextProtection() throws {
        var normalizer = UnicodeNormalizer()
        
        // Test bidirectional override characters
        let bidiText = "normal\u{202E}dangerous\u{202C}text"
        let result = normalizer.normalize(bidiText)
        let stats = normalizer.getStats()
        
        #expect(!result.contains("\u{202E}"), "Should remove RLO override")
        #expect(!result.contains("\u{202C}"), "Should remove PDF character")
        #expect(result == "normaldangeroustext", "Should remove all bidi formatting")
        #expect(stats.bidiReorderings > 0, "Should count bidi issues")
        #expect(stats.hasSecurityConcerns, "Should flag security concerns")
    }

    @Test("Security Configuration")
    func testSecurityConfiguration() throws {
        // Test with strict security configuration
        let strictConfig = UnicodeNormalizer.SecurityConfig(
            enableHomoglyphDetection: true,
            preventNormalizationAttacks: true,
            maxNormalizedLength: 10,
            detectBidiReordering: true
        )
        
        var normalizer = UnicodeNormalizer(securityConfig: strictConfig)
        
        // Test length limit protection
        let longText = String(repeating: "a", count: 20)
        let result = normalizer.normalize(longText)
        let stats = normalizer.getStats()
        
        #expect(result == longText, "Should return original when over length limit")
        #expect(stats.securityIssuesFound > 0, "Should flag security issue")
        
        // Test with disabled features
        let lenientConfig = UnicodeNormalizer.SecurityConfig(
            enableHomoglyphDetection: false,
            preventNormalizationAttacks: false,
            maxNormalizedLength: 100000,
            detectBidiReordering: false
        )
        
        var lenientNormalizer = UnicodeNormalizer(securityConfig: lenientConfig)
        let homoglyphText = "–∞—Å–µ"  // Cyrillic homoglyphs
        let lenientResult = lenientNormalizer.normalize(homoglyphText)
        let lenientStats = lenientNormalizer.getStats()
        
        #expect(lenientStats.homoglyphsDetected == 0, "Should not detect homoglyphs when disabled")
        #expect(!lenientStats.hasSecurityConcerns, "Should not flag security concerns when disabled")
    }

    @Test("Extended Character Support - CJK Extension")
    func testExtendedCharacterSupportCJK() throws {
        // Test with CJK Extension B characters
        let cjkText = "†ÄÄ†ÄÅ†ÄÇ"  // CJK Extension B characters
        let normalized = UnicodeNormalizer.normalizeForFE(cjkText)
        
        #expect(normalized == cjkText, "CJK Extension B characters should be preserved")
    }

    @Test("Extended Character Support - Private Use Area")
    func testExtendedCharacterSupportPrivateUse() throws {
        // Test with Private Use Area characters
        let privateUseText = "\u{E000}\u{E001}\u{E002}"
        let normalized = UnicodeNormalizer.normalizeForFE(privateUseText)
        
        #expect(normalized == privateUseText, "Private Use Area characters should be preserved")
    }

    @Test("Comprehensive Mixed Content Analysis")
    func testComprehensiveMixedContentAnalysis() throws {
        var normalizer = UnicodeNormalizer()
        
        // Complex text with multiple types of normalization needed
        let complexText = "Ôº®ÔΩÖÔΩåÔΩåÔΩè„ÄÄœÄ√óŒ±ÔºùŒ≤ÔºÅüòÄ\u{FE0F}–∞—Å–µ\u{202E}test"
        let result = normalizer.normalize(complexText)
        let stats = normalizer.getStats()
        
        // Verify all types of normalization occurred
        #expect(stats.fullwidthConversions > 0, "Should have full-width conversions")
        #expect(stats.mathSymbolNormalizations > 0, "Should have math symbol normalizations")
        #expect(stats.emojiNormalizations > 0, "Should have emoji normalizations")
        #expect(stats.homoglyphsDetected > 0, "Should detect homoglyphs")
        #expect(stats.bidiReorderings > 0, "Should detect bidi issues")
        #expect(stats.hasSecurityConcerns, "Should flag multiple security concerns")
        
        // Verify the result contains expected normalizations
        #expect(result.contains("Hello"), "Should normalize full-width characters")
        #expect(result.contains("pi"), "Should normalize œÄ")
        #expect(result.contains("*"), "Should normalize √ó")
        #expect(result.contains("alpha"), "Should normalize Œ±")
        #expect(result.contains("ace"), "Should normalize Cyrillic homoglyphs")
        #expect(!result.contains("\u{202E}"), "Should remove bidi override")
    }

    @Test("Normalization Analysis Enhanced")
    func testNormalizationAnalysisEnhanced() throws {
        let normalizer = UnicodeNormalizer()
        
        let testText = "Ôº°œÄüòÄ\u{FE0F}–∞—Å–µ\u{202E}"
        let analysis = normalizer.analyzeNormalization(testText)
        
        #expect(analysis.hasChanges, "Should detect changes")
        #expect(analysis.fullwidthCharactersConverted > 0, "Should count full-width characters")
        #expect(analysis.mathSymbolsNormalized > 0, "Should count math symbols")
        #expect(analysis.emojiCharactersNormalized > 0, "Should count emoji")
        #expect(analysis.homoglyphsDetected > 0, "Should count homoglyphs")
        #expect(analysis.bidiIssuesFound > 0, "Should count bidi issues")
        #expect(analysis.hasSecurityConcerns, "Should flag security concerns")
        
        let summary = analysis.summary
        #expect(summary.contains("full-width"), "Summary should mention full-width conversions")
        #expect(summary.contains("math symbol"), "Summary should mention math symbols")
        #expect(summary.contains("emoji"), "Summary should mention emoji")
        #expect(summary.contains("homoglyphs"), "Summary should mention homoglyphs")
        #expect(summary.contains("bidi"), "Summary should mention bidi issues")
    }

    @Test("String Extension Methods Enhanced")
    func testStringExtensionMethodsEnhanced() throws {
        // Use text with decomposable characters for NFC/NFD comparison
        let testTextWithAccent = "caf\u{00E9}"  // √© as composed character
        
        // Test individual normalization methods - compare at Unicode scalar level
        #expect(testTextWithAccent.normalizedNFC.unicodeScalars.count != testTextWithAccent.normalizedNFD.unicodeScalars.count, "NFC and NFD should have different scalar counts")
        #expect(testTextWithAccent.normalizedNFD.unicodeScalars.contains(UnicodeScalar(0x0301)!), "NFD should contain combining marks")
        
        let testText = "Ôº®ÔΩÖÔΩåÔΩåÔΩè œÄ"
        #expect(testText.normalizedNFKC.contains("Hello"), "NFKC should normalize full-width")
        #expect(testText.normalizedNFKD.contains("Hello"), "NFKD should normalize full-width")
        #expect(testText.normalizedFullwidth.contains("Hello"), "Should normalize full-width only")
        #expect(testText.normalizedMathSymbols.contains("pi"), "Should normalize math symbols")
        
        // Test with different forms and security configs
        let (normalized, stats) = testText.normalizedForFEWithStats(
            form: .nfkc,
            securityConfig: UnicodeNormalizer.SecurityConfig()
        )
        
        #expect(normalized.contains("Hello"), "Should normalize with specified form")
        #expect(stats.fullwidthConversions > 0, "Should track statistics")
    }

    @Test("Tokenizer Integration Enhanced")
    func testTokenizerIntegrationEnhanced() throws {
        // Test that the main Tokenizer now uses enhanced Unicode normalization
        let complexInput = "Â§âÊï∞„ÄÄÔº∂Ôº°Ôº≤„ÄÄÔºù„ÄÄœÄ„ÄÄ√ó„ÄÄÔºí"
        let tokenizer = Tokenizer(input: complexInput)
        
        let tokens = try tokenizer.tokenize()
        
        // Verify that tokens are properly normalized
        let identifierTokens = tokens.filter { $0.type == .identifier }
        let varToken = identifierTokens.first { $0.lexeme == "VAR" }
        #expect(varToken != nil, "Should find normalized VAR identifier")
        
        // The œÄ should be normalized to "pi" and tokenized as identifier
        let piToken = identifierTokens.first { $0.lexeme == "pi" }
        #expect(piToken != nil, "Should find normalized pi identifier")
    }
}
